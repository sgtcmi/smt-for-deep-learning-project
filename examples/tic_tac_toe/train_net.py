"""
Script to train a network using data generated by datagen and using tensorflow.keras

Command line arguments:
    Arg 1:  Size of data to generate, defaults to 10000, must eval to int
    Arg 2:  Number of epochs to train, defaults to 300, must eval to int
    Arg 3:  Ratio of training vs testing data, defaults to 0.6, must eval to float
"""

import sys
import os.path

from tensorflow.keras import models, layers, losses
import z3

from datagen import *
# This is so that if run as a script, we can import siblings and ancestors
if __name__ == "__main__":
    import sys
    sys.path.insert(1, "../..")

from utils.encode_dnn import *
from game_props import *


"""
Parse command line args
"""

if len(sys.argv) >= 2:
    num = eval(sys.argv[1])
else:
    num = 10000
if len(sys.argv) >= 3:
    epch = eval(sys.argv[2])
else:
    epch = 300
if len(sys.argv) >= 4:
    ratio = eval(sys.argv[3])
else:
    ratio = 0.6


"""
Generate Data
"""

data = gen_data(num, './out/data.val')


"""
Make and train DNN
"""

model_file = './out/model'

# Split training and testing data, prepare data
train_x = list(map(lambda x : [1 if c else 0 for c in x], [p[0] for p in data[:int(ratio*num)]]))
train_y = [([0, 1] if p[1] else [1, 0]) for p in data[:int(ratio*num)]]
test_x = list(map(lambda x : [1 if c else 0 for c in x], [p[0] for p in data[int(ratio*num):]]))
test_y = [([0, 1] if p[1] else [1, 0]) for p in data[int(ratio*num):]]


if os.path.isdir(model_file):
    dnn = models.load_model(model_file)
else:
    # Create a simple DNN model
    dnn = models.Sequential([
            layers.Input(shape = (36,)),
            layers.Dense(20, activation='relu'),
            layers.Dense(10, activation='relu'),
            layers.Dense(2, activation='relu'),])

    # We do not use the softmax normalization in the DNN itself. Instead, we bake a normalization into
    # the loss function by using the BinaryCrossentropy function with the from_logits flag set to True.
    # This will effectively apply softmax while calculating loss, but the model itself does not apply
    # softmax to the output.
    dnn.compile(loss = losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])

    # Train model
    dnn.fit(train_x, train_y, epochs=epch)

    # save model
    dnn.save(model_file)

# Test model
dnn.evaluate(test_x, test_y, verbose=2)
print(dnn.summary())

# Get weights
#print([(type(l.get_weights()), l.get_weights()) for l in dnn.layers])



"""
Verify properties
"""

from direct_check import *
from perm_check import *

# We get the weights and biases of the network
weights = [l.get_weights()[0].tolist() for l in dnn.layers]
biases =  [l.get_weights()[1].tolist() for l in dnn.layers]


# Build permutation
rot33 = [ (2 - i//3) + 3*(i%3) for i in range(9) ]
print('Perm for board, 9 items ', rot33)
perm = [ 3*rot33[i//3] + i%3 for i in range(27) ] + list(map(lambda x: x+27, rot33))
print('Permutation is: ', perm)

def input_constr(z3_inp, solv):
    z3_move = [ z3.Bool('b_%d'%i) for i in range(36) ]
    assert_valid_move(solv, z3_move)
    assert_input_encoding(solv, z3_move, z3_inp)

print('Direct Check')
res, mdl = direct_check(weights, biases, perm, input_constr)

if res:
    print('Verified')
else:
    print('Not verified, model: ', mdl)


# Exacly one of each of the triplets representing what move a cell has is one. We express this by
# saying that the sum of the values of those inputs is 1. Similarly, the sum of the inputs
# representing the move is 1.
lin_conds = [ [0]*(3*i) + [1, 1, 1] + [0]*(36-3*(i+1)) + [1] for i in range(9) ] + [ [0]*27 + [1]*10 ]


print('Perm Check')
res, mdl = perm_check(weights, biases, perm, lin_conds)

if res:
    print('Verified')
else:
    print('Not verified, model: ', mdl)
